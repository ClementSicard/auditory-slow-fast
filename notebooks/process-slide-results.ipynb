{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/cs7561/.local/lib/python3.8/site-packages (2.1.2)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/cs7561/.local/lib/python3.8/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/cs7561/.local/lib/python3.8/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: networkx in /home/cs7561/.local/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/cs7561/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/cs7561/.local/lib/python3.8/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/cs7561/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/cs7561/.local/lib/python3.8/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: fsspec in /home/cs7561/.local/lib/python3.8/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/cs7561/.local/lib/python3.8/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/cs7561/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from torch) (2.11.2)\n",
      "Requirement already satisfied: typing-extensions in /home/cs7561/.local/lib/python3.8/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/cs7561/.local/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/cs7561/.local/lib/python3.8/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/cs7561/.local/lib/python3.8/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: sympy in /home/cs7561/.local/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: filelock in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from torch) (3.0.12)\n",
      "Requirement already satisfied: triton==2.1.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/cs7561/.local/lib/python3.8/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/cs7561/.local/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/cs7561/.local/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: str):\n",
    "    df = pd.read_pickle(path)\n",
    "\n",
    "    verbs_probs = torch.from_numpy(df[\"verb_output\"])\n",
    "    verbs_labels = torch.from_numpy(df[\"verb_labels\"])\n",
    "    nouns_probs = torch.from_numpy(df[\"noun_output\"])\n",
    "    nouns_labels = torch.from_numpy(df[\"noun_labels\"])\n",
    "    \n",
    "    return verbs_probs, verbs_labels, nouns_probs, nouns_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create function to compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n",
    "\n",
    "\"\"\"Functions for computing metrics.\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def topks_correct(preds, labels, ks, inside_action_bounds=True, weight=None):\n",
    "    \"\"\"\n",
    "    Given the predictions, labels, and a list of top-k values, compute the\n",
    "    number of correct predictions for each top-k value.\n",
    "\n",
    "    Args:\n",
    "        preds (array): array of predictions. Dimension is batchsize\n",
    "            N x ClassNum.\n",
    "        labels (array): array of labels. Dimension is batchsize N.\n",
    "        ks (list): list of top-k values. For example, ks = [1, 5] correspods\n",
    "            to top-1 and top-5.\n",
    "\n",
    "    Returns:\n",
    "        topks_correct (list): list of numbers, where the `i`-th entry\n",
    "            corresponds to the number of top-`ks[i]` correct predictions.\n",
    "    \"\"\"\n",
    "    weight = torch.ones(preds.size(0))/preds.size(0) if weight==None else weight/torch.sum(weight)\n",
    "    assert preds.size(0) == labels.size(\n",
    "        0\n",
    "    ), \"Batch dim of predictions and labels must match\"\n",
    "    # Find the top max_k predictions for each sample\n",
    "    _top_max_k_vals, top_max_k_inds = torch.topk(\n",
    "        preds, max(ks), dim=1, largest=True, sorted=True\n",
    "    )\n",
    "    # (batch_size, max_k) -> (max_k, batch_size).\n",
    "    top_max_k_inds = top_max_k_inds.t()\n",
    "    # (batch_size, ) -> (max_k, batch_size).\n",
    "    if inside_action_bounds:\n",
    "        rep_max_k_labels = labels.view(1, -1).expand_as(top_max_k_inds.repeat(4,1))\n",
    "        top_max_k_correct = top_max_k_inds.eq(rep_max_k_labels)\n",
    "    else:\n",
    "        top_max_k_correct = torch.zeros_like(top_max_k_inds)\n",
    "        for label in labels.t():\n",
    "            rep_max_k_labels = label.view(1, -1).expand_as(top_max_k_inds)\n",
    "            top_max_k_correct_ = top_max_k_inds.eq(rep_max_k_labels)\n",
    "            top_max_k_correct |= top_max_k_correct_\n",
    "    # (i, j) = 1 if top i-th prediction for the j-th sample is correct.\n",
    "    # Compute the number of topk correct predictions for each k.\n",
    "    topks_correct = [\n",
    "        (weight*top_max_k_correct[:k, :]).contiguous().view(-1).float().sum() for k in ks\n",
    "    ]\n",
    "    return topks_correct\n",
    "\n",
    "\n",
    "def multitask_topks_correct(preds, labels, ks=(1,), inside_action_bounds=True, weight=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        preds: tuple(torch.FloatTensor), each tensor should be of shape\n",
    "            [batch_size, class_count], class_count can vary on a per task basis, i.e.\n",
    "            outputs[i].shape[1] can be different to outputs[j].shape[j].\n",
    "        labels: tuple(torch.LongTensor), each tensor should be of shape [batch_size]\n",
    "        ks: tuple(int), compute accuracy at top-k for the values of k specified\n",
    "            in this parameter.\n",
    "    Returns:\n",
    "        tuple(float), same length at topk with the corresponding accuracy@k in.\n",
    "    \"\"\"\n",
    "    weight = torch.ones(preds[0].size(0)) if weight is None else weight\n",
    "    num_vids = torch.sum(weight)\n",
    "    weight = weight/num_vids\n",
    "    max_k = int(np.max(ks))\n",
    "    task_count = len(preds)\n",
    "    batch_size = labels[0].size(0)\n",
    "    all_correct = torch.zeros(max_k, batch_size).type(torch.ByteTensor)\n",
    "    #if torch.cuda.is_available():\n",
    "    #    all_correct = all_correct.cuda()\n",
    "    for output, label in zip(preds, labels):\n",
    "        _, max_k_idx = output.topk(max_k, dim=1, largest=True, sorted=True)\n",
    "        # Flip batch_size, class_count as .view doesn't work on non-contiguous\n",
    "        max_k_idx = max_k_idx.t()\n",
    "        if inside_action_bounds:\n",
    "            correct_for_task = max_k_idx.repeat(4,1).eq(label.view(1, -1).expand_as(max_k_idx))\n",
    "        else:\n",
    "            correct_for_task = torch.zeros_like(max_k_idx)\n",
    "            for l in label.t():\n",
    "                correct_for_task_ = max_k_idx.eq(l.view(1, -1).expand_as(max_k_idx))\n",
    "                correct_for_task |= correct_for_task_\n",
    "        all_correct.add_(correct_for_task)\n",
    "    multitask_topks_correct = [\n",
    "        (weight*torch.ge((all_correct[:k]).float().sum(0), task_count)).float().sum(0) for k in ks\n",
    "    ]\n",
    "\n",
    "    return multitask_topks_correct\n",
    "\n",
    "\n",
    "def topk_errors(preds, labels, ks):\n",
    "    \"\"\"\n",
    "    Computes the top-k error for each k.\n",
    "    Args:\n",
    "        preds (array): array of predictions. Dimension is N.\n",
    "        labels (array): array of labels. Dimension is N.\n",
    "        ks (list): list of ks to calculate the top accuracies.\n",
    "    \"\"\"\n",
    "    num_topks_correct = topks_correct(preds, labels, ks)\n",
    "    return [(1.0 - x) * 100.0 for x in num_topks_correct]\n",
    "\n",
    "\n",
    "def topk_accuracies(preds, labels, ks, inside_action_bounds=True, weight = None):\n",
    "    \"\"\"\n",
    "    Computes the top-k accuracy for each k.\n",
    "    Args:\n",
    "        preds (array): array of predictions. Dimension is N.\n",
    "        labels (array): array of labels. Dimension is N.\n",
    "        ks (list): list of ks to calculate the top accuracies.\n",
    "    \"\"\"\n",
    "    num_topks_correct = topks_correct(preds, labels, ks, inside_action_bounds, weight)\n",
    "    return [x * 100.0 for x in num_topks_correct]\n",
    "\n",
    "\n",
    "def multitask_topk_accuracies(preds, labels, ks, inside_action_bounds=True, weight = None):\n",
    "    \"\"\"\n",
    "    Computes the top-k accuracy for each k.\n",
    "    Args:\n",
    "        preds (array): array of predictions. Dimension is N.\n",
    "        labels (array): array of labels. Dimension is N.\n",
    "        ks (list): list of ks to calculate the top accuracies.\n",
    "   \"\"\"\n",
    "    num_multitask_topks_correct = multitask_topks_correct(preds, labels, ks, inside_action_bounds, weight)\n",
    "    return [x * 100.0 for x in num_multitask_topks_correct]\n",
    "\n",
    "\n",
    "def multi_top1_action_accuracy_per_action(verb_probs, noun_probs, verb_labels, noun_labels):\n",
    "    \"\"\"\n",
    "    Computes the top1 accuracy for the multi-action task.\n",
    "    \"\"\"\n",
    "    n_correct = 0\n",
    "    n_classes = 0\n",
    "    \n",
    "    verb_preds = torch.argmax(verb_probs, dim=1)\n",
    "    noun_preds = torch.argmax(noun_probs, dim=1)\n",
    "\n",
    "    N = noun_preds.shape[0]\n",
    "\n",
    "    for i in range(N):\n",
    "        unique_labels = set(zip(verb_labels[i], noun_labels[i]))\n",
    "        n_classes += len(unique_labels)\n",
    "\n",
    "        for verb_label, noun_label in unique_labels:\n",
    "            n_correct += int((verb_preds[i] == verb_label) and (noun_preds[i] == noun_label))\n",
    "\n",
    "    return n_correct / n_classes * 100\n",
    "\n",
    "\n",
    "def top1_accuracy_per_action(probs, labels):\n",
    "    \"\"\"\n",
    "    Computes the top1 accuracy for the multi-action task.\n",
    "    \"\"\"\n",
    "    n_correct = 0\n",
    "    n_classes = 0\n",
    "    \n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "    N = preds.shape[0]\n",
    "\n",
    "    for i in range(N):\n",
    "        unique_labels = labels[i].unique()\n",
    "        n_classes += unique_labels.shape[0]\n",
    "\n",
    "        for label in unique_labels:\n",
    "            n_correct += int(preds[i] == label)\n",
    "\n",
    "    return n_correct / n_classes * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    verbs_probs: torch.Tensor,\n",
    "    verbs_labels: torch.Tensor,\n",
    "    nouns_probs: torch.Tensor,\n",
    "    nouns_labels: torch.Tensor,\n",
    "    per_action_instance: bool = False,\n",
    ") -> Tuple[float, float, float]:\n",
    "    if per_action_instance:\n",
    "        action_acc = multi_top1_action_accuracy_per_action(\n",
    "            verb_probs=verbs_probs,\n",
    "            noun_probs=nouns_probs,\n",
    "            verb_labels=verbs_labels,\n",
    "            noun_labels=nouns_labels,\n",
    "        )\n",
    "        verb_acc = top1_accuracy_per_action(\n",
    "            verbs_probs, verbs_labels\n",
    "        )\n",
    "        noun_acc = top1_accuracy_per_action(\n",
    "            nouns_probs, nouns_labels\n",
    "        )\n",
    "    else:\n",
    "        action_acc = multitask_topk_accuracies(\n",
    "            (verbs_probs, nouns_probs),\n",
    "            (verbs_labels, nouns_labels),\n",
    "            (1,),\n",
    "            inside_action_bounds=False,\n",
    "        )[0]\n",
    "        verb_acc = topk_accuracies(\n",
    "            verbs_probs,\n",
    "            verbs_labels,\n",
    "            (1,),\n",
    "            per_action_instance,\n",
    "        )[0]\n",
    "        noun_acc = topk_accuracies(\n",
    "            nouns_probs,\n",
    "            nouns_labels,\n",
    "            (1,),\n",
    "            per_action_instance,\n",
    "        )[0]\n",
    "\n",
    "    return action_acc, verb_acc, noun_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get results for `WHOLE_VIDEO` mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_wv, v_wv, n_wv = compute_metrics(\n",
    "    verbs_probs=verbs_probs,\n",
    "    verbs_labels=verbs_labels,\n",
    "    nouns_probs=nouns_probs,\n",
    "    nouns_labels=nouns_labels,\n",
    "    per_action_instance=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get results for `IN_ACTION_BOUNDS` mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows where all elements are -1\n",
    "labels_to_discard_verbs = torch.all(verbs_labels == -1, dim=1)\n",
    "labels_to_discard_nouns = torch.all(nouns_labels == -1, dim=1)\n",
    "\n",
    "assert nouns_labels[~labels_to_discard_nouns].shape == verbs_labels[~labels_to_discard_verbs].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_ab, v_ab, n_ab = compute_metrics(\n",
    "    verbs_probs=verbs_probs[~labels_to_discard_verbs],\n",
    "    verbs_labels=verbs_labels[~labels_to_discard_verbs],\n",
    "    nouns_probs=nouns_probs[~labels_to_discard_nouns],\n",
    "    nouns_labels=nouns_labels[~labels_to_discard_nouns],\n",
    "    per_action_instance=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Window: 0.5\n",
      "\tSlide per action      - A: 2.38, V: 18.18, N: 6.38\n",
      "\tSlide labeled footage - A: 2.61, V: 18.93, N: 6.57\n",
      "\tSlide full footage    - A: 1.80, V: 13.09, N: 4.54\n",
      "====================================================================================================\n",
      "Window: 1.0\n",
      "\tSlide per action      - A: 5.00, V: 25.17, N: 10.32\n",
      "\tSlide labeled footage - A: 5.33, V: 26.19, N: 10.61\n",
      "\tSlide full footage    - A: 3.69, V: 18.12, N: 7.34\n",
      "====================================================================================================\n",
      "Window: 2.0\n",
      "\tSlide per action      - A: 8.40, V: 35.35, N: 14.88\n",
      "\tSlide labeled footage - A: 8.79, V: 36.81, N: 15.31\n",
      "\tSlide full footage    - A: 6.10, V: 25.56, N: 10.63\n",
      "====================================================================================================\n",
      "Window: 4.0\n",
      "\tSlide per action      - A: 4.97, V: 26.92, N: 10.56\n",
      "\tSlide labeled footage - A: 5.19, V: 28.03, N: 10.86\n",
      "\tSlide full footage    - A: 3.61, V: 19.50, N: 7.56\n",
      "====================================================================================================\n",
      "Window: 8.0\n",
      "\tSlide per action      - A: 1.51, V: 17.46, N: 5.57\n",
      "\tSlide labeled footage - A: 1.62, V: 18.18, N: 5.73\n",
      "\tSlide full footage    - A: 1.13, V: 12.69, N: 4.00\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "res_path = \"../runs/asf-original-slide/scores/{}/slide/validation.pkl\"\n",
    "\n",
    "print(\"=\" * 100)\n",
    "windows = [0.5, 1.0, 2.0, 4.0, 8.0]\n",
    "for win in windows:\n",
    "    assert os.path.exists(res_path.format(str(win)))\n",
    "    print(f\"Window: {win}\")\n",
    "    verbs_probs, verbs_labels, nouns_probs, nouns_labels = load_data(res_path.format(str(win)))\n",
    "    \n",
    "    # Compute results for whole video\n",
    "    a_wv, v_wv, n_wv = compute_metrics(\n",
    "        verbs_probs=verbs_probs,\n",
    "        verbs_labels=verbs_labels,\n",
    "        nouns_probs=nouns_probs,\n",
    "        nouns_labels=nouns_labels,\n",
    "        per_action_instance=False,\n",
    "    )\n",
    "    \n",
    "    # Compute results for inside action bounds\n",
    "    labels_to_discard_verbs = torch.all(verbs_labels == -1, dim=1)\n",
    "    labels_to_discard_nouns = torch.all(nouns_labels == -1, dim=1)\n",
    "\n",
    "    assert nouns_labels[~labels_to_discard_nouns].shape == verbs_labels[~labels_to_discard_verbs].shape\n",
    "    \n",
    "    a_ab, v_ab, n_ab = compute_metrics(\n",
    "        verbs_probs=verbs_probs[~labels_to_discard_verbs],\n",
    "        verbs_labels=verbs_labels[~labels_to_discard_verbs],\n",
    "        nouns_probs=nouns_probs[~labels_to_discard_nouns],\n",
    "        nouns_labels=nouns_labels[~labels_to_discard_nouns],\n",
    "        per_action_instance=False,\n",
    "    )\n",
    "    \n",
    "    # Compute per action\n",
    "    a_pi, v_pi, n_pi = compute_metrics(\n",
    "        verbs_probs=verbs_probs[~labels_to_discard_verbs],\n",
    "        verbs_labels=verbs_labels[~labels_to_discard_verbs],\n",
    "        nouns_probs=nouns_probs[~labels_to_discard_nouns],\n",
    "        nouns_labels=nouns_labels[~labels_to_discard_nouns],\n",
    "        per_action_instance=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\tSlide per action      - A: {a_pi:.2f}, V: {v_pi:.2f}, N: {n_pi:.2f}\")    \n",
    "    print(f\"\\tSlide labeled footage - A: {a_ab:.2f}, V: {v_ab:.2f}, N: {n_ab:.2f}\")\n",
    "    print(f\"\\tSlide full footage    - A: {a_wv:.2f}, V: {v_wv:.2f}, N: {n_wv:.2f}\")\n",
    "    print(\"=\" * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
