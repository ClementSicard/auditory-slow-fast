{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from audio_slowfast.utils.metrics import multitask_topk_accuracies, topk_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>narration_timestamp</th>\n",
       "      <th>start_timestamp</th>\n",
       "      <th>stop_timestamp</th>\n",
       "      <th>start_frame</th>\n",
       "      <th>stop_frame</th>\n",
       "      <th>narration</th>\n",
       "      <th>verb</th>\n",
       "      <th>verb_class</th>\n",
       "      <th>noun</th>\n",
       "      <th>noun_class</th>\n",
       "      <th>all_nouns</th>\n",
       "      <th>all_noun_classes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>narration_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P01_11_0</th>\n",
       "      <td>P01</td>\n",
       "      <td>P01_11</td>\n",
       "      <td>00:00:00.560</td>\n",
       "      <td>00:00:00.00</td>\n",
       "      <td>00:00:01.89</td>\n",
       "      <td>1</td>\n",
       "      <td>113</td>\n",
       "      <td>take plate</td>\n",
       "      <td>take</td>\n",
       "      <td>0</td>\n",
       "      <td>plate</td>\n",
       "      <td>2</td>\n",
       "      <td>[plate]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P01_11_1</th>\n",
       "      <td>P01</td>\n",
       "      <td>P01_11</td>\n",
       "      <td>00:00:01.700</td>\n",
       "      <td>00:00:01.56</td>\n",
       "      <td>00:00:02.45</td>\n",
       "      <td>93</td>\n",
       "      <td>147</td>\n",
       "      <td>put down plate</td>\n",
       "      <td>put-down</td>\n",
       "      <td>1</td>\n",
       "      <td>plate</td>\n",
       "      <td>2</td>\n",
       "      <td>[plate]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P01_11_10</th>\n",
       "      <td>P01</td>\n",
       "      <td>P01_11</td>\n",
       "      <td>00:00:48.500</td>\n",
       "      <td>00:00:49.15</td>\n",
       "      <td>00:00:50.95</td>\n",
       "      <td>2949</td>\n",
       "      <td>3057</td>\n",
       "      <td>take paper</td>\n",
       "      <td>take</td>\n",
       "      <td>0</td>\n",
       "      <td>paper</td>\n",
       "      <td>49</td>\n",
       "      <td>[paper]</td>\n",
       "      <td>[49]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P01_11_100</th>\n",
       "      <td>P01</td>\n",
       "      <td>P01_11</td>\n",
       "      <td>00:05:27.840</td>\n",
       "      <td>00:05:27.28</td>\n",
       "      <td>00:05:31.97</td>\n",
       "      <td>19636</td>\n",
       "      <td>19918</td>\n",
       "      <td>wash cloth</td>\n",
       "      <td>wash</td>\n",
       "      <td>2</td>\n",
       "      <td>cloth</td>\n",
       "      <td>17</td>\n",
       "      <td>[cloth]</td>\n",
       "      <td>[17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P01_11_101</th>\n",
       "      <td>P01</td>\n",
       "      <td>P01_11</td>\n",
       "      <td>00:05:26.840</td>\n",
       "      <td>00:05:27.37</td>\n",
       "      <td>00:05:29.86</td>\n",
       "      <td>19642</td>\n",
       "      <td>19791</td>\n",
       "      <td>take cloth</td>\n",
       "      <td>take</td>\n",
       "      <td>0</td>\n",
       "      <td>cloth</td>\n",
       "      <td>17</td>\n",
       "      <td>[cloth]</td>\n",
       "      <td>[17]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             participant_id video_id narration_timestamp start_timestamp  \\\n",
       "narration_id                                                               \n",
       "P01_11_0                P01   P01_11        00:00:00.560     00:00:00.00   \n",
       "P01_11_1                P01   P01_11        00:00:01.700     00:00:01.56   \n",
       "P01_11_10               P01   P01_11        00:00:48.500     00:00:49.15   \n",
       "P01_11_100              P01   P01_11        00:05:27.840     00:05:27.28   \n",
       "P01_11_101              P01   P01_11        00:05:26.840     00:05:27.37   \n",
       "\n",
       "             stop_timestamp  start_frame  stop_frame       narration  \\\n",
       "narration_id                                                           \n",
       "P01_11_0        00:00:01.89            1         113      take plate   \n",
       "P01_11_1        00:00:02.45           93         147  put down plate   \n",
       "P01_11_10       00:00:50.95         2949        3057      take paper   \n",
       "P01_11_100      00:05:31.97        19636       19918      wash cloth   \n",
       "P01_11_101      00:05:29.86        19642       19791      take cloth   \n",
       "\n",
       "                  verb  verb_class   noun  noun_class all_nouns  \\\n",
       "narration_id                                                      \n",
       "P01_11_0          take           0  plate           2   [plate]   \n",
       "P01_11_1      put-down           1  plate           2   [plate]   \n",
       "P01_11_10         take           0  paper          49   [paper]   \n",
       "P01_11_100        wash           2  cloth          17   [cloth]   \n",
       "P01_11_101        take           0  cloth          17   [cloth]   \n",
       "\n",
       "             all_noun_classes  \n",
       "narration_id                   \n",
       "P01_11_0                  [2]  \n",
       "P01_11_1                  [2]  \n",
       "P01_11_10                [49]  \n",
       "P01_11_100               [17]  \n",
       "P01_11_101               [17]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_path = \"../data/epic-kitchens-100-annotations/EPIC_100_validation.pkl\"\n",
    "annotations = pd.read_pickle(annotations_path)\n",
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "a_path = \"../audio-gru-cm.pkl\"\n",
    "v_path = \"../sf-cm.pkl\"\n",
    "\n",
    "a_df = pd.read_pickle(a_path)\n",
    "v_df = pd.read_pickle(v_path)\n",
    "\n",
    "mm_df_sum = a_df.copy()\n",
    "del mm_df_sum[\"verb_output\"]\n",
    "del mm_df_sum[\"noun_output\"]\n",
    "\n",
    "mm_df_sum[\"verb_output\"] = v_df[\"verb_output\"] + a_df[\"verb_output\"]\n",
    "mm_df_sum[\"noun_output\"] = v_df[\"noun_output\"] + a_df[\"noun_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioSlowFastGRU:\n",
      "\tAction Acc@1: 17.06\n",
      "\tVerb Acc@1: 47.39\n",
      "\tNoun Acc@1: 24.24\n",
      "\tAction Acc@5: 44.30\n",
      "\tVerb Acc@5: 80.28\n",
      "\tNoun Acc@5: 49.49\n",
      "SlowFast:\n",
      "\tAction Acc@1: 38.54\n",
      "\tVerb Acc@1: 65.56\n",
      "\tNoun Acc@1: 50.02\n",
      "\tAction Acc@5: 70.28\n",
      "\tVerb Acc@5: 90.00\n",
      "\tNoun Acc@5: 75.62\n",
      "MMLate-SlowFastGRU:\n",
      "\tAction Acc@1: 39.04\n",
      "\tVerb Acc@1: 66.31\n",
      "\tNoun Acc@1: 50.37\n",
      "\tAction Acc@5: 70.45\n",
      "\tVerb Acc@5: 90.33\n",
      "\tNoun Acc@5: 75.70\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy_at_k(df, labels, ks):\n",
    "    for k in ks:\n",
    "        verb_acc = topk_accuracies(\n",
    "            torch.from_numpy(df[\"verb_output\"]),\n",
    "            torch.from_numpy(labels[\"verb_class\"].values),\n",
    "            (k,),\n",
    "        )[0].item()\n",
    "        noun_acc = topk_accuracies(\n",
    "            torch.from_numpy(df[\"noun_output\"]),\n",
    "            torch.from_numpy(labels[\"noun_class\"].values),\n",
    "            (k,),\n",
    "        )[0].item()\n",
    "\n",
    "        action_acc = multitask_topk_accuracies(\n",
    "            (torch.from_numpy(df[\"verb_output\"]), torch.from_numpy(df[\"noun_output\"])),\n",
    "            (torch.from_numpy(labels[\"verb_class\"].values), torch.from_numpy(labels[\"noun_class\"].values)),\n",
    "            (k,),\n",
    "        )[0].item()\n",
    "\n",
    "        print(f\"\\tAction Acc@{k}: {action_acc:.2f}\")\n",
    "        print(f\"\\tVerb Acc@{k}: {verb_acc:.2f}\")\n",
    "        print(f\"\\tNoun Acc@{k}: {noun_acc:.2f}\")\n",
    "\n",
    "\n",
    "print(\"AudioSlowFastGRU:\")\n",
    "compute_accuracy_at_k(a_df, annotations, (1, 5))\n",
    "\n",
    "print(\"SlowFast:\")\n",
    "compute_accuracy_at_k(v_df, annotations, (1, 5))\n",
    "\n",
    "print(\"MMLate-SlowFastGRU:\")\n",
    "compute_accuracy_at_k(mm_df_sum, annotations, (1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.verb_class.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.282051282051282"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/78 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_slowfast.models.audio_model_builder import AudioSlowFast\n",
    "from audio_slowfast.config.defaults import get_cfg\n",
    "\n",
    "\n",
    "cfg_path = \"../models/asf/config/asf-augment.yaml\"\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(cfg_path)\n",
    "\n",
    "cfg.NUM_GPUS = 0\n",
    "\n",
    "model = AudioSlowFast(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 26,882,293\n"
     ]
    }
   ],
   "source": [
    "# Print the number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "import torch\n",
    "from fvcore.common.file_io import PathManager\n",
    "from fvcore.nn.activation_count import activation_count\n",
    "from fvcore.nn.flop_count import flop_count\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "import audio_slowfast.utils.multiprocessing as mpu\n",
    "from audio_slowfast.datasets.utils import pack_pathway_output\n",
    "from audio_slowfast.models.batchnorm_helper import SubBatchNorm2d\n",
    "\n",
    "\n",
    "def check_nan_losses(loss):\n",
    "    \"\"\"\n",
    "    Determine whether the loss is NaN (not a number).\n",
    "    Args:\n",
    "        loss (loss): loss to check whether is NaN.\n",
    "    \"\"\"\n",
    "    if math.isnan(loss):\n",
    "        raise RuntimeError(\"ERROR: Got NaN losses {}\".format(datetime.now()))\n",
    "\n",
    "\n",
    "def params_count(model, ignore_bn=False):\n",
    "    \"\"\"\n",
    "    Compute the number of parameters.\n",
    "    Args:\n",
    "        model (model): model to count the number of parameters.\n",
    "    \"\"\"\n",
    "    if not ignore_bn:\n",
    "        return np.sum([p.numel() for p in model.parameters()]).item()\n",
    "    else:\n",
    "        count = 0\n",
    "        for m in model.modules():\n",
    "            if not isinstance(m, nn.BatchNorm2d):\n",
    "                for p in m.parameters(recurse=False):\n",
    "                    count += p.numel()\n",
    "    return count\n",
    "\n",
    "\n",
    "def gpu_mem_usage():\n",
    "    \"\"\"\n",
    "    Compute the GPU memory usage for the current device (GB).\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        mem_usage_bytes = torch.cuda.max_memory_allocated()\n",
    "    else:\n",
    "        mem_usage_bytes = 0\n",
    "    return mem_usage_bytes / 1024**3\n",
    "\n",
    "\n",
    "def cpu_mem_usage():\n",
    "    \"\"\"\n",
    "    Compute the system memory (RAM) usage for the current device (GB).\n",
    "    Returns:\n",
    "        usage (float): used memory (GB).\n",
    "        total (float): total memory (GB).\n",
    "    \"\"\"\n",
    "    vram = psutil.virtual_memory()\n",
    "    usage = (vram.total - vram.available) / 1024**3\n",
    "    total = vram.total / 1024**3\n",
    "\n",
    "    return usage, total\n",
    "\n",
    "\n",
    "def _get_model_analysis_input(cfg):\n",
    "    \"\"\"\n",
    "    Return a dummy input for model analysis with batch size 1. The input is\n",
    "        used for analyzing the model (counting flops and activations etc.).\n",
    "    Args:\n",
    "        cfg (CfgNode): configs. Details can be found in\n",
    "            slowfast/config/defaults.py\n",
    "\n",
    "    Returns:\n",
    "        inputs: the input for model analysis.\n",
    "    \"\"\"\n",
    "    spectrogram_dimension = 1\n",
    "    input_tensors = torch.rand(\n",
    "        spectrogram_dimension,\n",
    "        cfg.AUDIO_DATA.NUM_FRAMES,\n",
    "        cfg.AUDIO_DATA.NUM_FREQUENCIES,\n",
    "    )\n",
    "    model_inputs = pack_pathway_output(cfg, input_tensors)\n",
    "    for i in range(len(model_inputs)):\n",
    "        model_inputs[i] = model_inputs[i].unsqueeze(0)\n",
    "        if cfg.NUM_GPUS:\n",
    "            model_inputs[i] = model_inputs[i].cuda(non_blocking=True)\n",
    "\n",
    "    inputs = (model_inputs,)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def get_model_stats(model, cfg, mode):\n",
    "    \"\"\"\n",
    "    Compute statistics for the current model given the config.\n",
    "    Args:\n",
    "        model (model): model to perform analysis.\n",
    "        cfg (CfgNode): configs. Details can be found in\n",
    "            slowfast/config/defaults.py\n",
    "        mode (str): Options include `flop` or `activation`. Compute either flop\n",
    "            (gflops) or activation count (mega).\n",
    "\n",
    "    Returns:\n",
    "        float: the total number of count of the given model.\n",
    "    \"\"\"\n",
    "    assert mode in [\n",
    "        \"flop\",\n",
    "        \"activation\",\n",
    "    ], \"'{}' not supported for model analysis\".format(mode)\n",
    "    if mode == \"flop\":\n",
    "        model_stats_fun = flop_count\n",
    "    elif mode == \"activation\":\n",
    "        model_stats_fun = activation_count\n",
    "\n",
    "    # Set model to evaluation mode for analysis.\n",
    "    # Evaluation mode can avoid getting stuck with sync batchnorm.\n",
    "    model_mode = model.training\n",
    "    model.eval()\n",
    "    inputs = _get_model_analysis_input(cfg)\n",
    "    count_dict, *_ = model_stats_fun(model, inputs)\n",
    "    count = sum(count_dict.values())\n",
    "    model.train(model_mode)\n",
    "    return count\n",
    "\n",
    "\n",
    "def log_model_info(model, cfg):\n",
    "    \"\"\"\n",
    "    Log info, includes number of parameters, gpu usage, gflops and activation count.\n",
    "        The model info is computed when the model is in validation mode.\n",
    "    Args:\n",
    "        model (model): model to log the info.\n",
    "        cfg (CfgNode): configs. Details can be found in\n",
    "            slowfast/config/defaults.py\n",
    "    \"\"\"\n",
    "    logger.info(\"Model:\\n{}\".format(model))\n",
    "    logger.info(\"Params: {:,}\".format(params_count(model)))\n",
    "    logger.info(\"Mem: {:,} MB\".format(gpu_mem_usage()))\n",
    "    logger.info(\"Flops: {:,} G\".format(get_model_stats(model, cfg, \"flop\")))\n",
    "    logger.info(\"Activations: {:,} M\".format(get_model_stats(model, cfg, \"activation\")))\n",
    "    logger.info(\"nvidia-smi\")\n",
    "    os.system(\"nvidia-smi\")\n",
    "\n",
    "\n",
    "def is_eval_epoch(cfg, cur_epoch):\n",
    "    \"\"\"\n",
    "    Determine if the model should be evaluated at the current epoch.\n",
    "    Args:\n",
    "        cfg (CfgNode): configs. Details can be found in\n",
    "            slowfast/config/defaults.py\n",
    "        cur_epoch (int): current epoch.\n",
    "    \"\"\"\n",
    "    if cur_epoch + 1 == cfg.SOLVER.MAX_EPOCH:\n",
    "        return True\n",
    "\n",
    "    return (cur_epoch + 1) % cfg.TRAIN.EVAL_PERIOD == 0\n",
    "\n",
    "\n",
    "def plot_input(tensor, bboxes=(), texts=(), path=\"./tmp_vis.png\"):\n",
    "    \"\"\"\n",
    "    Plot the input tensor with the optional bounding box and save it to disk.\n",
    "    Args:\n",
    "        tensor (tensor): a tensor with shape of `NxCxHxW`.\n",
    "        bboxes (tuple): bounding boxes with format of [[x, y, h, w]].\n",
    "        texts (tuple): a tuple of string to plot.\n",
    "        path (str): path to the image to save to.\n",
    "    \"\"\"\n",
    "    tensor = tensor.float()\n",
    "    tensor = tensor - tensor.min()\n",
    "    tensor = tensor / tensor.max()\n",
    "    f, ax = plt.subplots(nrows=1, ncols=tensor.shape[0], figsize=(50, 20))\n",
    "    for i in range(tensor.shape[0]):\n",
    "        ax[i].axis(\"off\")\n",
    "        ax[i].imshow(tensor[i].permute(1, 2, 0))\n",
    "        # ax[1][0].axis('off')\n",
    "        if bboxes is not None and len(bboxes) > i:\n",
    "            for box in bboxes[i]:\n",
    "                x1, y1, x2, y2 = box\n",
    "                ax[i].vlines(x1, y1, y2, colors=\"g\", linestyles=\"solid\")\n",
    "                ax[i].vlines(x2, y1, y2, colors=\"g\", linestyles=\"solid\")\n",
    "                ax[i].hlines(y1, x1, x2, colors=\"g\", linestyles=\"solid\")\n",
    "                ax[i].hlines(y2, x1, x2, colors=\"g\", linestyles=\"solid\")\n",
    "\n",
    "        if texts is not None and len(texts) > i:\n",
    "            ax[i].text(0, 0, texts[i])\n",
    "    f.savefig(path)\n",
    "\n",
    "\n",
    "def aggregate_sub_bn_stats(module):\n",
    "    \"\"\"\n",
    "    Recursively find all SubBN modules and aggregate sub-BN stats.\n",
    "    Args:\n",
    "        module (nn.Module)\n",
    "    Returns:\n",
    "        count (int): number of SubBN module found.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for child in module.children():\n",
    "        if isinstance(child, SubBatchNorm2d):\n",
    "            child.aggregate_stats()\n",
    "            count += 1\n",
    "        else:\n",
    "            count += aggregate_sub_bn_stats(child)\n",
    "    return count\n",
    "\n",
    "\n",
    "def launch_job(cfg, init_method, func, daemon=False):\n",
    "    \"\"\"\n",
    "    Run 'func' on one or more GPUs, specified in cfg\n",
    "    Args:\n",
    "        cfg (CfgNode): configs. Details can be found in\n",
    "            slowfast/config/defaults.py\n",
    "        init_method (str): initialization method to launch the job with multiple\n",
    "            devices.\n",
    "        func (function): job to run on GPU(s)\n",
    "        daemon (bool): The spawned processesâ€™ daemon flag. If set to True,\n",
    "            daemonic processes will be created\n",
    "    \"\"\"\n",
    "    if cfg.NUM_GPUS > 1:\n",
    "        torch.multiprocessing.spawn(\n",
    "            mpu.run,\n",
    "            nprocs=cfg.NUM_GPUS,\n",
    "            args=(\n",
    "                cfg.NUM_GPUS,\n",
    "                func,\n",
    "                init_method,\n",
    "                cfg.SHARD_ID,\n",
    "                cfg.NUM_SHARDS,\n",
    "                cfg.DIST_BACKEND,\n",
    "                cfg,\n",
    "            ),\n",
    "            daemon=daemon,\n",
    "        )\n",
    "    else:\n",
    "        func(cfg=cfg)\n",
    "\n",
    "\n",
    "def get_class_names(path, parent_path=None, subset_path=None):\n",
    "    \"\"\"\n",
    "    Read json file with entries {classname: index} and return\n",
    "    an array of class names in order.\n",
    "    If parent_path is provided, load and map all children to their ids.\n",
    "    Args:\n",
    "        path (str): path to class ids json file.\n",
    "            File must be in the format {\"class1\": id1, \"class2\": id2, ...}\n",
    "        parent_path (Optional[str]): path to parent-child json file.\n",
    "            File must be in the format {\"parent1\": [\"child1\", \"child2\", ...], ...}\n",
    "        subset_path (Optional[str]): path to text file containing a subset\n",
    "            of class names, separated by newline characters.\n",
    "    Returns:\n",
    "        class_names (list of strs): list of class names.\n",
    "        class_parents (dict): a dictionary where key is the name of the parent class\n",
    "            and value is a list of ids of the children classes.\n",
    "        subset_ids (list of ints): list of ids of the classes provided in the\n",
    "            subset file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with PathManager.open(path, \"r\") as f:\n",
    "            class2idx = json.load(f)\n",
    "    except Exception as err:\n",
    "        print(\"Fail to load file from {} with error {}\".format(path, err))\n",
    "        return\n",
    "\n",
    "    max_key = max(class2idx.values())\n",
    "    class_names = [None] * (max_key + 1)\n",
    "\n",
    "    for k, i in class2idx.items():\n",
    "        class_names[i] = k\n",
    "\n",
    "    class_parent = None\n",
    "    if parent_path is not None and parent_path != \"\":\n",
    "        try:\n",
    "            with PathManager.open(parent_path, \"r\") as f:\n",
    "                d_parent = json.load(f)\n",
    "        except EnvironmentError as err:\n",
    "            print(\"Fail to load file from {} with error {}\".format(parent_path, err))\n",
    "            return\n",
    "        class_parent = {}\n",
    "        for parent, children in d_parent.items():\n",
    "            indices = [class2idx[c] for c in children if class2idx.get(c) is not None]\n",
    "            class_parent[parent] = indices\n",
    "\n",
    "    subset_ids = None\n",
    "    if subset_path is not None and subset_path != \"\":\n",
    "        try:\n",
    "            with PathManager.open(subset_path, \"r\") as f:\n",
    "                subset = f.read().split(\"\\n\")\n",
    "                subset_ids = [class2idx[name] for name in subset if class2idx.get(name) is not None]\n",
    "        except EnvironmentError as err:\n",
    "            print(\"Fail to load file from {} with error {}\".format(subset_path, err))\n",
    "            return\n",
    "\n",
    "    return class_names, class_parent, subset_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.332613887999999"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_stats(model, cfg, \"flop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auditory-slow-fast-4TKuBWLx-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
